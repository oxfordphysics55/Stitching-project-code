{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a3c1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "722e1b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data/', \n",
    "                                        train=True,\n",
    "                                        download=True, \n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                       train=False,\n",
    "                                       download=True, \n",
    "                                       transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=4,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd7b0f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show some sample images and their labels\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Convert tensor to numpy for visualization\n",
    "images = images.numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "# Plot the first 5 images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(images[i].transpose(1, 2, 0))  # Transpose to (height, width, channels)\n",
    "    ax.set_title(f\"Label: {labels[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "917d24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "559d28e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = [3]\n",
    "\n",
    "# Load the pretrained ResNet-18 model\n",
    "model1 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to match CIFAR-10 (10 classes)\n",
    "model1.fc = nn.Linear(model1.fc.in_features, 10)\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "\n",
    "# Load the pretrained ResNet-18 model\n",
    "model2 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to match CIFAR-10 (10 classes)\n",
    "model2.fc = nn.Linear(model2.fc.in_features, 10)\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2607dc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2761405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (CrossEntropyLoss) and optimizer (SGD)\n",
    "weight_decay = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=0.001, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21656b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6335\n",
      "Epoch [2/10], Loss: 1.3005\n",
      "Epoch [3/10], Loss: 1.1452\n",
      "Epoch [4/10], Loss: 1.0556\n",
      "Epoch [5/10], Loss: 0.9699\n",
      "Epoch [6/10], Loss: 0.9250\n",
      "Epoch [7/10], Loss: 0.8642\n",
      "Epoch [8/10], Loss: 0.8088\n",
      "Epoch [9/10], Loss: 0.7646\n",
      "Epoch [10/10], Loss: 0.7265\n"
     ]
    }
   ],
   "source": [
    "# Define loss function (CrossEntropyLoss) and optimizer (SGD)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(trainloader):.4f}\")\n",
    "    \n",
    "    \n",
    "torch.save(model1, 'ResNet18_1.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "042394d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 9.9364\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     l1_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnorm(p, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model2\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00005\u001b[39m \u001b[38;5;241m*\u001b[39m l1_loss  \u001b[38;5;66;03m# Scale L1 loss properly\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Apply pruning gradually\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This trains a model which is hopefully simpler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization\n",
    "\n",
    "# L1 Regularization function\n",
    "def l1_regularization(model2, l1_weight=0.0001):\n",
    "    l1_norm = 0\n",
    "    for param in model2.parameters():\n",
    "        l1_norm += torch.sum(torch.abs(param))\n",
    "    return l1_weight * l1_norm\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.0001, momentum=0.2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "num_epochs = 3\n",
    "\n",
    "def gradual_pruning(model2, start_epoch, total_epochs, amount_per_step):\n",
    "    for module in model2.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount_per_step * (start_epoch / total_epochs))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    \n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # L1 regularization\n",
    "        l1_loss = sum(torch.norm(p, 1) for p in model2.parameters())\n",
    "        loss += 0.00005 * l1_loss  # Scale L1 loss properly\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Apply pruning gradually\n",
    "    gradual_pruning(model2, epoch + 1, num_epochs, amount_per_step=0.01)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "torch.save(model2, 'ResNet18_2b.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db323929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005662924470231175"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_sharpness(model1, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b4101a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.29902856113813"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient_norm(model1, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08429aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332424736"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_model_numpy(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "582146b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023111710533166464"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_sharpness(model2, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e2a0523a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.687221837881655"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient_norm(model2, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "570ade0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331963192"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_model_numpy(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "39a2ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_sharpness(model, loss_fn, testloader, epsilon=1e-3):\n",
    "    \"\"\"Computes the sharpness measure by perturbing model parameters.\"\"\"\n",
    "    model.eval()\n",
    "    total_sharpness = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    original_params = {name: p.clone() for name, p in model.named_parameters()}\n",
    "\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(next(model.parameters()).device), target.to(next(model.parameters()).device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        original_loss = loss_fn(output, target).item()\n",
    "\n",
    "        # Perturb parameters\n",
    "        for p in model.parameters():\n",
    "            p.data += epsilon * torch.randn_like(p)\n",
    "\n",
    "        perturbed_output = model(data)\n",
    "        perturbed_loss = loss_fn(perturbed_output, target).item()\n",
    "\n",
    "        # Restore original parameters\n",
    "        for name, p in model.named_parameters():\n",
    "            p.data = original_params[name]\n",
    "\n",
    "        total_sharpness += perturbed_loss - original_loss\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_sharpness / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "def compute_gradient_norm(model, loss_fn, testloader):\n",
    "    \"\"\"Computes the average gradient norm over the test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_norm = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(next(model.parameters()).device), target.to(next(model.parameters()).device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        batch_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "        total_norm += batch_norm.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_norm / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "def compress_model_numpy(model):\n",
    "    weights = np.concatenate([p.cpu().detach().numpy().flatten() for p in model.parameters()])\n",
    "    model_bytes = pickle.dumps(weights)\n",
    "    compressed = gzip.compress(model_bytes)\n",
    "    return len(compressed) * 8  # Size in bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "98756c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Stitching): StitchingLayer(\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1x1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class StitchingLayer(nn.Module):\n",
    "    def __init__(self, C1, C2):\n",
    "        super(StitchingLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(C1)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=C1, out_channels=C2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(C2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
    "        self.Stitching = StitchingLayer(64, 64)  # Added StitchingLayer\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.Stitching(x)  # Pass through StitchingLayer\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model3 = ResNet18(num_classes=10)\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0ad00669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the state_dict for each model\n",
    "state_dict_1 = model1.state_dict()\n",
    "state_dict_2 = model2.state_dict()\n",
    "state_dict_3 = model3.state_dict()\n",
    "\n",
    "# Identify keys for copying\n",
    "keys_1 = list(state_dict_1.keys())\n",
    "keys_2 = list(state_dict_2.keys())\n",
    "keys_3 = list(state_dict_3.keys())\n",
    "\n",
    "# Mapping function to handle different key formats\n",
    "def get_matching_key(key, source_keys):\n",
    "    if key in source_keys:\n",
    "        return key\n",
    "    if key.startswith(\"0.\") and key[2:] in source_keys:\n",
    "        return key[2:]  # Remove '0.' prefix if necessary\n",
    "    if key.endswith(\"_orig\") and key[:-5] in source_keys:\n",
    "        return key[:-5]  # Remove '_orig' suffix if necessary\n",
    "    return None\n",
    "\n",
    "for key in keys_3:\n",
    "    match_key_1 = get_matching_key(key, keys_1)\n",
    "    match_key_2 = get_matching_key(key, keys_2)\n",
    "    \n",
    "    if \"layer1\" in key or \"conv1\" in key or \"bn1\" in key or \"maxpool\" in key:\n",
    "        if match_key_2:\n",
    "            state_dict_3[key] = state_dict_2[match_key_2]\n",
    "    elif \"layer2\" in key or \"layer3\" in key or \"layer4\" in key or \"fc\" in key:\n",
    "        if match_key_1:\n",
    "            state_dict_3[key] = state_dict_1[match_key_1]\n",
    "    elif \"stitching\" in key:\n",
    "        print(f\"Skipping {key}, keeping original initialization for StitchingLayer\")\n",
    "\n",
    "# Load the modified state_dict into model3\n",
    "model3.load_state_dict(state_dict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a4ed7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(state_dict_3.keys())  # Check available keys\n",
    "#print(state_dict_1.keys())  # Check keys in model1\n",
    "#print(state_dict_2.keys())  # Check keys in model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d06b6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stitching_layer(model3, dataloader, criterion, optimizer, num_epochs=5):\n",
    "    # Loop through all model parameters\n",
    "    for name, param in model3.named_parameters():\n",
    "        if 'Stitching' in name:  # Ensure we target the stitching layer by checking its name\n",
    "            param.requires_grad = True  # Allow gradients for the stitching layer\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze other layers\n",
    "\n",
    "    model3.to(device)\n",
    "    model3.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model3(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backpropagate to calculate gradients\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate loss for this epoch\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "    \n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "083f6d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model3.parameters(), lr=0.0005, momentum=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d19b59d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.6446880706596376\n",
      "Epoch 2/5, Loss: 2.630164176950455\n",
      "Epoch 3/5, Loss: 2.6253169284629823\n",
      "Epoch 4/5, Loss: 2.629913808584213\n",
      "Epoch 5/5, Loss: 2.622219603962898\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train_stitching_layer(model3, trainloader, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c12a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016867da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5fd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822e664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e704aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ad7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ad019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "494a7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "\n",
    "class StitchingLayer(nn.Module):\n",
    "    def __init__(self, C1, C2):\n",
    "        super(StitchingLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(C1)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=C1, out_channels=C2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(C2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "def get_last_conv_out_channels(layer):\n",
    "    \"\"\"Helper function to get output channels from the last Conv2d layer in a given layer/block.\"\"\"\n",
    "    if isinstance(layer, nn.Sequential):  \n",
    "        last_block = list(layer.children())[-1]  # Get the last BasicBlock\n",
    "        if isinstance(last_block, nn.Module):  \n",
    "            for sub_layer in reversed(list(last_block.children())):  \n",
    "                if isinstance(sub_layer, nn.Conv2d):  \n",
    "                    return sub_layer.out_channels\n",
    "    return None\n",
    "\n",
    "def get_first_conv_in_channels(layer):\n",
    "    \"\"\"Helper function to get input channels from the first Conv2d layer in a given layer/block.\"\"\"\n",
    "    if isinstance(layer, nn.Sequential):  \n",
    "        first_block = list(layer.children())[0]  # Get the first BasicBlock\n",
    "        if isinstance(first_block, nn.Module):  \n",
    "            for sub_layer in list(first_block.children()):  \n",
    "                if isinstance(sub_layer, nn.Conv2d):  \n",
    "                    return sub_layer.in_channels\n",
    "    return None\n",
    "\n",
    "def insert_stitching_layer(model, target_layer_name):\n",
    "    \"\"\"\n",
    "    Inserts a stitching layer after the specified layer in ResNet.\n",
    "    Automatically detects C1 (output channels of target layer) and C2 (input channels of next layer).\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The original ResNet model.\n",
    "        target_layer_name (str): The layer after which to insert the stitching layer.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Modified ResNet model with the stitching layer inserted.\n",
    "    \"\"\"\n",
    "    # Create a deep copy of the model so we don't modify the original\n",
    "    new_model = copy.deepcopy(model)\n",
    "\n",
    "    # Get all named layers in order\n",
    "    layers = list(new_model.named_children())\n",
    "\n",
    "    # Find C1 (output channels of the target layer) and C2 (input channels of the next layer)\n",
    "    C1, C2 = None, None\n",
    "    for i, (name, module) in enumerate(layers):\n",
    "        if name == target_layer_name:\n",
    "            C1 = get_last_conv_out_channels(module)  # Get C1 from the last conv layer in target block\n",
    "            if i + 1 < len(layers):  # Ensure there's a next layer\n",
    "                C2 = get_first_conv_in_channels(layers[i + 1][1])  # Get C2 from the first conv layer in next block\n",
    "            break\n",
    "\n",
    "    if C1 is None or C2 is None:\n",
    "        raise ValueError(f\"Could not determine C1 and C2 for layer '{target_layer_name}'. Check layer names.\")\n",
    "\n",
    "    # Create new layer sequence with stitching inserted\n",
    "    new_layers = nn.Sequential()\n",
    "    for name, module in layers:\n",
    "        new_layers.add_module(name, module)  # Add existing layer\n",
    "        if name == target_layer_name:  # If this is the target layer\n",
    "            stitching = StitchingLayer(C1, C2)\n",
    "            new_layers.add_module(\"stitching_layer\", stitching)  # Insert stitching layer\n",
    "\n",
    "    # Rebuild the model with modified layers\n",
    "    new_model = nn.Sequential(new_layers)\n",
    "\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1c79afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (stitching_layer): StitchingLayer(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1x1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model3 = models.resnet18(pretrained=True)\n",
    "target_layer_name = \"layer1\"\n",
    "model3 = insert_stitching_layer(model3, target_layer_name)\n",
    "\n",
    "# Print model to verify placement\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "de27929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def transfer_weights(model3, model1, model2, stitching_layer_name=\"stitching_layer\"):\n",
    "    model3_state_dict = model3.state_dict()\n",
    "    model1_state_dict = model1.state_dict()\n",
    "    model2_state_dict = model2.state_dict()\n",
    "    \n",
    "    stitching_found = False\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    for name, param in model3_state_dict.items():\n",
    "        if stitching_layer_name in name:\n",
    "            stitching_found = True\n",
    "        \n",
    "        if not stitching_found:  # Before stitching layer, copy from model1\n",
    "            if name in model1_state_dict:\n",
    "                new_state_dict[name] = model1_state_dict[name].clone()\n",
    "            else:\n",
    "                raise KeyError(f\"{name} not found in model1\")\n",
    "        else:  # After stitching layer, copy from model2\n",
    "            if name in model2_state_dict:\n",
    "                new_state_dict[name] = model2_state_dict[name].clone()\n",
    "            else:\n",
    "                raise KeyError(f\"{name} not found in model2\")\n",
    "    \n",
    "    model3.load_state_dict(new_state_dict)\n",
    "    return model3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0b43dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stitching_layer(model3, dataloader, criterion, optimizer, num_epochs=5):\n",
    "    for name, param in model3.named_parameters():\n",
    "        stitching_layer_name =\"stitching_layer\"\n",
    "        if stitching_layer_name in name:\n",
    "            param.requires_grad = True  # Keep stitching layer trainable\n",
    "        else:\n",
    "            param.requires_grad = False  # Freeze all other layers\n",
    "    model3.to(device)\n",
    "    model3.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model3(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "    \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5fff13db",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x1 and 512x1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_stitching_layer(model3, trainloader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[119], line 17\u001b[0m, in \u001b[0;36mtrain_stitching_layer\u001b[0;34m(model3, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model3(inputs)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x1 and 512x1000)"
     ]
    }
   ],
   "source": [
    "train_stitching_layer(model3, trainloader, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b211968",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "80d10fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on 10000 test images: 16.26%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy test\n",
    "total_correct = 0\n",
    "total_images = 0\n",
    "confusion_matrix = np.zeros([10,10], int)\n",
    "net = model1\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        for i, l in enumerate(labels):\n",
    "            confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
    "\n",
    "model_accuracy = total_correct / total_images * 100\n",
    "print('Model accuracy on {0} test images: {1:.2f}%'.format(total_images, model_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710eafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8655e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67370b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89f7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc301a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
