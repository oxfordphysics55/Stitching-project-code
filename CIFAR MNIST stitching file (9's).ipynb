{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0406bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prerequisitics for loading data, run once\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "06066bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Original training set size: 50000\n",
      "Training set size without class 9: 45000\n",
      "Training set size with only class 9: 5000\n"
     ]
    }
   ],
   "source": [
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the full CIFAR-10 dataset\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data/', train=True, download=True, transform=transform)\n",
    "full_testset = torchvision.datasets.CIFAR10(root='./data/', train=False, download=True, transform=transform)\n",
    "\n",
    "# Filter dataset: Exclude class 9\n",
    "filtered_train_data_no_9 = [(img, label) for img, label in full_trainset if label != 9]\n",
    "filtered_train_images_no_9, filtered_train_labels_no_9 = zip(*filtered_train_data_no_9)\n",
    "\n",
    "# Filter dataset: Only class 9\n",
    "filtered_train_data_only_9 = [(img, label) for img, label in full_trainset if label == 9]\n",
    "filtered_train_images_only_9, filtered_train_labels_only_9 = zip(*filtered_train_data_only_9)\n",
    "\n",
    "# Create dataset without class 9\n",
    "trainset_no_9 = torch.utils.data.TensorDataset(\n",
    "    torch.stack(filtered_train_images_no_9), torch.tensor(filtered_train_labels_no_9)\n",
    ")\n",
    "trainloader_no_9 = torch.utils.data.DataLoader(trainset_no_9, batch_size=4, shuffle=True)\n",
    "\n",
    "# Create dataset with only class 9\n",
    "trainset_only_9 = torch.utils.data.TensorDataset(\n",
    "    torch.stack(filtered_train_images_only_9), torch.tensor(filtered_train_labels_only_9)\n",
    ")\n",
    "trainloader_only_9 = torch.utils.data.DataLoader(trainset_only_9, batch_size=4, shuffle=True)\n",
    "\n",
    "# Filter testset: Exclude class 9\n",
    "filtered_test_data_no_9 = [(img, label) for img, label in full_testset if label != 9]\n",
    "filtered_test_images_no_9, filtered_test_labels_no_9 = zip(*filtered_test_data_no_9)\n",
    "\n",
    "# Filter dataset: Only class 9\n",
    "filtered_test_data_only_9 = [(img, label) for img, label in full_testset if label == 9]\n",
    "filtered_test_images_only_9, filtered_test_labels_only_9 = zip(*filtered_test_data_only_9)\n",
    "\n",
    "# Create test dataset without class 9\n",
    "testset_no_9 = torch.utils.data.TensorDataset(\n",
    "    torch.stack(filtered_test_images_no_9), torch.tensor(filtered_test_labels_no_9)\n",
    ")\n",
    "testloader_no_9 = torch.utils.data.DataLoader(testset_no_9, batch_size=4, shuffle=True)\n",
    "\n",
    "# Create dataset with only class 9\n",
    "testset_only_9 = torch.utils.data.TensorDataset(\n",
    "    torch.stack(filtered_test_images_only_9), torch.tensor(filtered_test_labels_only_9)\n",
    ")\n",
    "testloader_only_9 = torch.utils.data.DataLoader(testset_only_9, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# Normal trainloader (all classes)\n",
    "trainloader_full = torch.utils.data.DataLoader(full_trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Normal testloader (unchanged)\n",
    "testloader = torch.utils.data.DataLoader(full_testset, batch_size=4, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Original training set size: {len(full_trainset)}\")\n",
    "print(f\"Training set size without class 9: {len(trainset_no_9)}\")\n",
    "print(f\"Training set size with only class 9: {len(trainset_only_9)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bfe7e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 84)\n",
    "        #self.fc4 = nn.Linear(84, 84)\n",
    "        self.fc5 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        x = F.relu(self.fc3(x), inplace=True)\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "33b3509d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/cr9dv6h106v9sddn3p0_pt200000gn/T/ipykernel_56115/2428143689.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net1 = torch.load('model_unlucky_9_1d_full.pth')\n",
      "/var/folders/w5/cr9dv6h106v9sddn3p0_pt200000gn/T/ipykernel_56115/2428143689.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net2c = torch.load('model_unlucky_9_2c_full.pth')\n",
      "/var/folders/w5/cr9dv6h106v9sddn3p0_pt200000gn/T/ipykernel_56115/2428143689.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net2e = torch.load('model_unlucky_9_2e_full.pth')\n",
      "/var/folders/w5/cr9dv6h106v9sddn3p0_pt200000gn/T/ipykernel_56115/2428143689.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net2f = torch.load('model2e_full.pth')\n"
     ]
    }
   ],
   "source": [
    "#This loads the two trained model. \n",
    "# Model 1d has FC1 as the special layer\n",
    "net1 = torch.load('model_unlucky_9_1d_full.pth')\n",
    "net2c = torch.load('model_unlucky_9_2c_full.pth')\n",
    "net2e = torch.load('model_unlucky_9_2e_full.pth')\n",
    "net2f = torch.load('model2e_full.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "160078ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose which model to work with\n",
    "net1 = net2e\n",
    "net2 = net2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "28b6ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for airplane 0: 71.30%\n",
      "Accuracy for automobile 1: 70.50%\n",
      "Accuracy for bird 2: 46.20%\n",
      "Accuracy for cat 3: 33.60%\n",
      "Accuracy for deer 4: 57.80%\n",
      "Accuracy for dog 5: 50.90%\n",
      "Accuracy for frog 6: 80.30%\n",
      "Accuracy for horse 7: 64.80%\n",
      "Accuracy for ship 8: 78.90%\n",
      "Accuracy for truck 9: 74.80%\n"
     ]
    }
   ],
   "source": [
    "#This evaluates the model performance based on whatever data is loaded\n",
    "def calculate_class_accuracies(model, testloader, classes):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize variables to track correct predictions for each class\n",
    "    class_correct = [0] * len(classes)\n",
    "    class_total = [0] * len(classes)\n",
    "\n",
    "    # Iterate through the test dataset\n",
    "    with torch.no_grad():  # Don't compute gradients during evaluation\n",
    "        for data in testloader:\n",
    "            images, labels = data  # Get images and corresponding labels\n",
    "            outputs = model(images)  # Get model outputs\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "\n",
    "            # Update correct predictions for each class\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i].item() == label:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "    # Print accuracy for each class\n",
    "    for i in range(len(classes)):\n",
    "        accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "        print(f'Accuracy for {classes[i]}: {accuracy:.2f}%')\n",
    "\n",
    "# Example usage: Assuming testloader and class names are defined\n",
    "# The 'classes' should be a list of class names for your dataset (e.g., CIFAR-10 classes)\n",
    "\n",
    "# Define class names (CIFAR-10 example)\n",
    "classes = ['airplane 0', 'automobile 1', 'bird 2', 'cat 3', 'deer 4', 'dog 5', \n",
    "                 'frog 6', 'horse 7', 'ship 8', 'truck 9']\n",
    "\n",
    "# Call the function to calculate and print class-wise accuracies\n",
    "calculate_class_accuracies(net2f, testloader, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2589e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stitching network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 84)\n",
    "        self.stitch = nn.Linear(84, 84)\n",
    "        #self.fc4 = nn.Linear(84, 84)\n",
    "        self.fc5 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.stitch(x)\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net3 = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8f8036b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state_dict for each model\n",
    "state_dict_1 = net1.state_dict()\n",
    "state_dict_2 = net2.state_dict()\n",
    "state_dict_3 = net3.state_dict()\n",
    "\n",
    "# Iterate over the state_dict keys and copy weights\n",
    "keys = list(state_dict_3.keys())\n",
    "for i, key in enumerate(keys):\n",
    "    if i < 10:\n",
    "        # First half\n",
    "        state_dict_3[key] = state_dict_2[key]\n",
    "    elif i > 11:\n",
    "        # Second half\n",
    "        state_dict_3[key] = state_dict_1[key]\n",
    "\n",
    "# Load the modified state_dict into model_C\n",
    "net3.load_state_dict(state_dict_3)\n",
    "net = net3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8102ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([4, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "net3.eval()\n",
    "\n",
    "# Example: Test with some data\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Perform inference\n",
    "outputs = net3(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted:', predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "22f040c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e6217c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.322\n",
      "[1,  4000] loss: 0.989\n",
      "[1,  6000] loss: 0.933\n",
      "[1,  8000] loss: 0.916\n",
      "[1, 10000] loss: 0.874\n",
      "[1, 12000] loss: 0.872\n",
      "[2,  2000] loss: 0.867\n",
      "[2,  4000] loss: 0.882\n",
      "[2,  6000] loss: 0.880\n",
      "[2,  8000] loss: 0.853\n",
      "[2, 10000] loss: 0.864\n",
      "[2, 12000] loss: 0.861\n",
      "Finished Training.\n",
      "Model accuracy on 10000 test images: 63.68%\n",
      "Model accuracy on test set with only class 9: 66.70%\n",
      "Model accuracy on test set without class 9: 63.34%\n"
     ]
    }
   ],
   "source": [
    "    epochs = 2\n",
    "    \n",
    "    # Freeze all layers initially\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    #Unfreeze stitching layer\n",
    "    for param in net.stitch.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader_full, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training.')\n",
    "    \n",
    "    evaluate_model_full(net3, testloader)\n",
    "    evaluate_model_only_9(net3, testloader_only_9)\n",
    "    evaluate_model_no_9(net3, testloader_no_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd13ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "790d956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 compressed size: 2052000 bits\n",
      "Model 2 compressed size: 2056048 bits\n",
      "Model 3 compressed size: 2267600 bits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "def compress_model_numpy(model):\n",
    "    weights = np.concatenate([p.cpu().detach().numpy().flatten() for p in model.parameters()])\n",
    "    model_bytes = pickle.dumps(weights)\n",
    "    compressed = gzip.compress(model_bytes)\n",
    "    return len(compressed) * 8  # Size in bits\n",
    "\n",
    "# Test with both models\n",
    "complexity_1 = compress_model_numpy(net1)\n",
    "complexity_2 = compress_model_numpy(net2)\n",
    "complexity_3 = compress_model_numpy(net3)\n",
    "\n",
    "print(f\"Model 1 compressed size: {complexity_1} bits\")\n",
    "print(f\"Model 2 compressed size: {complexity_2} bits\")\n",
    "print(f\"Model 3 compressed size: {complexity_3} bits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5c526b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model, loss_fn, testloader):\n",
    "    \"\"\"Computes the average gradient norm over the test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_norm = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(next(model.parameters()).device), target.to(next(model.parameters()).device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        batch_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "        total_norm += batch_norm.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_norm / num_batches if num_batches > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4d20768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.438407219076157"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient_norm(net1, criterion, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4dde0c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.127162860943633"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient_norm(net2, criterion, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "76418986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.589355911933258"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient_norm(net3, criterion, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d4c4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_full(model, testloader):\n",
    "    model.eval() \n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    confusion_matrix = np.zeros([10, 10], int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            for i, l in enumerate(labels):\n",
    "                confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    print(f'Model accuracy on {total_images} test images: {model_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fa7d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_no_9(model, testloader_no_9):\n",
    "    model.eval() \n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    confusion_matrix = np.zeros([10, 10], int)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader_no_9:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            for i, l in enumerate(labels):\n",
    "                confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
    "\n",
    "    no_9_accuracy = total_correct / total_images * 100\n",
    "    print(f'Model accuracy on test set without class 9: {no_9_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc60f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_only_9(model, testloader_only_9):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    confusion_matrix = np.zeros([10, 10], int)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader_only_9:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            for i, l in enumerate(labels):\n",
    "                confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
    "\n",
    "    only_9_accuracy = total_correct / total_images * 100\n",
    "    print(f'Model accuracy on test set with only class 9: {only_9_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d243c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
